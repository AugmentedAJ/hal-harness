# Agent Evaluation Harness

## Table of Contents
1. [Setup](#setup)
2. [How to run existing agents as baselines](#how-to-run-existing-agents-as-baselines)
3. [How to add new agents](#how-to-add-new-agents)

   3.1. [SWE-bench Benchmark](#swe-bench-benchmark)
   
   3.2. [USACO Benchmark](#usaco-benchmark)

## Setup

To clone this repository and initialize the benchmark and agent submodules, follow these steps:

1. **Clone the main repository:**
   ```bash
   git clone --recursive https://github.com/benediktstroebl/agent-eval-harness.git
   cd agent-eval-harness
   ```

2. **Make sure poetry is installed:**
   ```bash
   pip install poetry
   ```

3. **Install package:**
   ```bash
   pip install -e .
   ```

3. **Save .env with your API keys**
   ```bash
   cp .env.template .env
   ```
   Now add your personal API keys to the ```.env``` file.


## How to run existing agents as baselines

To demonstrate the simplest example of how to implement and run an agent with this harness, you can take a look at the `agents/swebench_example_agent/`. It can be evaluated on SWE-bench Verified with the following command: 

   ```bash
   agent-eval --benchmark swebench_verified --agent_dir agents/swebench_example_agent/ --agent_function agent.run --agent_name "SWE-bench Verified Example Agent (gpt-4o-mini)"
   ```

Certainly! I'll restructure the section to provide separate instructions for each benchmark while maintaining a similar outline. Here's the updated section:

## How to add new agents

### SWE-bench Benchmark

1. **Create a new directory** for your agent in the `agents/` directory.

2. **Implement the agent function** with the following signature:

   ```python
   def run(tasks: Dict[str, Dict]) -> Dict[str, Dict]:
       # Your SWE-bench agent implementation here
       pass
   ```

3. **Input format:** The `tasks` parameter is a dictionary where each key is a task identifier and each value is a dictionary:

   ```python
   {
       "task_id": {
           "repo": str,
           "instance_id": str,
           "base_commit": str,
           "patch": str,
           "test_patch": str,
           "problem_statement": str,
           "hints_text": str,
           "created_at": str,
           "version": str,
           "FAIL_TO_PASS": str,
           "PASS_TO_PASS": str,
           "environment_setup_commit": str
       }
   }
   ```

4. **Output format:** Your agent function should return a dictionary with the same keys as the input:

   ```python
   {
       "task_id": {
           "instance_id": str,  # Same as input
           # Same keys as input
           "model_patch": str,  # The patch generated by your agent to fix the issue
           "model_name_or_path": str 
       }
   }
   ```

5. **Expose the agent function:** Ensure your agent function is importable as `from agent import run`.

6. **Test your agent:** Before we run your agent with the full benchmark we will use a simple test task provided in the `SWEBenchBenchmark` class to validate your agent. So feel free to run your agent as shown in `Step 7` to debug. 

7. **Run the evaluation:**

   ```bash
   agent-eval --benchmark [swebench_verified|swebench_lite] --agent_dir agents/your_agent_directory/ --agent_function agent.run --agent_name "Your SWE-bench Agent Name"
   ```

### USACO Benchmark

1. **Create a new directory** for your agent in the `agents/` directory.

2. **Implement the agent function** with the following signature:

   ```python
   def run(tasks: Dict[str, Dict]) -> Dict[str, Dict]:
       # Your USACO agent implementation here
       pass
   ```

3. **Input format:** The `tasks` parameter is a dictionary where each key is a problem identifier and each value is a dictionary:

   ```python
   {
       "problem_id": {
           "name": str,
           "problem_link": str,
           "test_data_link": str,
           "solution_link": str,
           "contest_link": str,
           "inner_contest_link": str,
           "problem_level": str,
           "cp_id": str,
           "description": str,
           "num_tests": int,
           "solution": str,
           "runtime_limit": int,
           "memory_limit": int,
           "samples": List[Dict[str, str]],
           "description_no_samples": str,
           "num_samples": int,
           "solution_python3": str,
           "solution_english": str
       }
   }
   ```

4. **Output format:** Your agent function should return a dictionary with the same keys as the input:

   ```python
   {
       "problem_id": {
           # same keys as in input
           "response": str  # The solution code generated by your agent
       }
   }
   ```

5. **Expose the agent function:** Ensure your agent function is importable as `from agent import run`.

6. **Test your agent:** Before we run your agent with the full benchmark we will use a simple test task provided in the `USACOBenchmark` class to validate your agent. So feel free to run your agent as shown in `Step 7` to debug. 

7. **Run the evaluation:**

   ```bash
   agent-eval --benchmark usaco --agent_dir agents/your_agent_directory/ --agent_function agent.run --agent_name "Your USACO Agent Name"
   ```




